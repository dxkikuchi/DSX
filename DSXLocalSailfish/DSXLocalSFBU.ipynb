{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><center><b>Predicting Outdoor Equipment Purchases<br><br> with Data Science Experience Local<br><br> on the IBM Integrated Analytics System</b></center></font></th>\n",
    "   </tr>\n",
    "   <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/blob/master/spark/product-line-prediction/images/products_graphics.png?raw=true\" alt=\"Icon\"> </th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides steps and code to load data from the IBM Integrated Analytics System (IIAS), explore this data, create a predictive model, deploy and start scoring with new data. In addition, it introduces basic data cleansing and exploration, pipeline creation, model training, model persistence, model deployment, predictions and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 2.0 and Apache® Spark 2.0.\n",
    "\n",
    "The data is publically available and contains anonymous outdoor equipment purchases.  It can be downloaded here [GoSales Transactions](https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600) but has already been loaded on IIAS.  Based on this data, interest in product lines (golf accessories, camping equipment, etc.) will be predicted based on a consumer's gender, age, marital status and job.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Load IIAS data into an Apache® Spark DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create an Apache® Spark machine learning pipeline.\n",
    "-  Train and evaluate a model.\n",
    "-  Persist a pipeline and model.\n",
    "-  Deploy a model for online scoring.\n",
    "-  Score with sample data.\n",
    "-  Explore and visualize prediction results using the plotly package.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Load and explore data](#load)\n",
    "2.\t[Create spark ml model](#model)\n",
    "3.\t[Persist model](#persistence)\n",
    "4.\t[Predict locally and visualize](#visualization)\n",
    "5.\t[Deploy and score](#scoring)\n",
    "7.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 1. Load and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will load the data as an Apache® Spark DataFrame and perform basic exploration.  First, a SparkSession will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GoSales data on IIAS will be loaded into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data = spark.read.format('jdbc').options(\n",
    "          url='jdbc:db2://dashdb-entry-yp-dal09-09.services.dal.bluemix.net:50000/BLUDB',\n",
    "          driver='com.ibm.db2.jcc.DB2Driver',\n",
    "          dbtable='dash7268.GOSALES',\n",
    "          user='dash7268',\n",
    "          password='mV@$e7q4RPzL').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explore the loaded data by using the following Apache® Spark DataFrame methods:\n",
    "-  print schema\n",
    "-  print top ten records\n",
    "-  count all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data contains five fields. PRODUCT_LINE field is the one we would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data set contains 60252 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 2. Create an Apache® Spark machine learning model\n",
    "\n",
    "In this section you will learn how to prepare data, create an Apache® Spark machine learning pipeline, and train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Prepare data\n",
    "\n",
    "In this subsection you will split your data into: train, test and predict datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitted_data = df_data.randomSplit([0.8, 0.18, 0.02], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print \"Number of training records: \" + str(train_data.count())\n",
    "print \"Number of testing records : \" + str(test_data.count())\n",
    "print \"Number of prediction records : \" + str(predict_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our data has been successfully split into three datasets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Create pipeline and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will create an Apache® Spark machine learning pipeline and then train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step you need to import the Apache® Spark machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, convert all the string fields to numeric ones by using the StringIndexer transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringIndexer_label = StringIndexer(inputCol=\"PRODUCT_LINE\", outputCol=\"label\").fit(df_data)\n",
    "stringIndexer_prof = StringIndexer(inputCol=\"PROFESSION\", outputCol=\"PROFESSION_IX\")\n",
    "stringIndexer_gend = StringIndexer(inputCol=\"GENDER\", outputCol=\"GENDER_IX\")\n",
    "stringIndexer_mar = StringIndexer(inputCol=\"MARITAL_STATUS\", outputCol=\"MARITAL_STATUS_IX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, create a feature vector by combining all features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=[\"GENDER_IX\", \"AGE\", \"MARITAL_STATUS_IX\", \"PROFESSION_IX\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define estimators you want to use for classification. Random Forest is used in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, bring indexed labels back to original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the pipeline now. A pipeline consists of transformers and an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **train data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_rf = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **model accuracy** now. To evaluate the model, use **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model_rf.transform(test_data)\n",
    "evaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluatorRF.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This model could be tuned for greater accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"persistence\"></a>\n",
    "## 3. Persist model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to store your pipeline and model by using Python client libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, you must import client libraries.\n",
    "\n",
    "**Note**: Apache® Spark 2.0 or higher is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from repository.mlrepositoryclient import MLRepositoryClient\n",
    "from repository.mlrepositoryartifact import MLRepositoryArtifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the service path and create a machine learning repository client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443'\n",
    "ml_repository_client = MLRepositoryClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model artifact (abstraction layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_artifact = MLRepositoryArtifact(model_rf, training_data=train_data, name=\"Product Line Prediction\")\n",
    "model_artifact.meta.add(\"authorName\", \"Data Scientist Dude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Save pipeline and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will learn how to save pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model = ml_repository_client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get saved model metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Use *meta.available_props()* to get the list of available props."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_model.meta.available_props()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"modelType: \" + saved_model.meta.prop(\"modelType\")\n",
    "print \"trainingDataSchema: \" + str(saved_model.meta.prop(\"trainingDataSchema\"))\n",
    "print \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\n",
    "print \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\n",
    "print \"label: \" + saved_model.meta.prop(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: **modelVersionHref** is the unique model indentifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will learn how to load back saved model from specified instance of Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print for example model name to make sure that model has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print str(loadedModelArtifact.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the name is correct. You have successfully saved and loaded the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualization\"></a>\n",
    "## 4. Predict locally and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will score test data using the loaded model and visualize the prediction results with plotly package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Make local prediction using previously loaded model and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will score *predict_data* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = loadedModelArtifact.model_instance().transform(predict_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the results by calling the *show()* method on the predictions DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tabulating a count, you can see which product line is the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Sample visualization of data with Plotly package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection you will explore prediction results with Plotly, which is an online analytics and data visualization tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you experience errors in this next code cell, you may need to install the required packages.  This can be done by issuing the following but only needs to be run one time.\n",
    "\n",
    "!pip install plotly --user <BR>\n",
    "!pip install cufflinks==0.8.2 --user\n",
    "<br><br>\n",
    "Import Plotly and other required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "sys.path.append(\"\".join([os.environ[\"HOME\"]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Apache Spark DataFrame to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_pdf = predictions.select(\"prediction\", \"predictedLabel\", \"GENDER\", \"AGE\", \"PROFESSION\", \"MARITAL_STATUS\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a pie chart that shows the predicted product-line interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cumulative_stats = predictions_pdf.groupby(['predictedLabel']).count()\n",
    "\n",
    "product_data = [go.Pie(\n",
    "            labels=cumulative_stats.index,\n",
    "            values=cumulative_stats['GENDER'],\n",
    "    )]\n",
    "\n",
    "product_layout = go.Layout(\n",
    "    title='Predicted product line client interest distribution',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=product_data, layout=product_layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data set, you might want to do some analysis of the mean AGE per product line by using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_data = [go.Bar(\n",
    "            y=predictions_pdf.groupby(['predictedLabel']).mean()[\"AGE\"],\n",
    "            x=cumulative_stats.index\n",
    "            \n",
    "    )]\n",
    "\n",
    "age_layout = go.Layout(\n",
    "    title='Mean AGE per predicted product line',\n",
    "    xaxis=dict(\n",
    "        title = \"Product Line\",\n",
    "        showline=False,),\n",
    "    yaxis=dict(\n",
    "        title = \"Mean AGE\",\n",
    "        ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=age_data, layout=age_layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on bar plot you created, you might make the following conclusion: The mean age for clients that are interested in golf equipment is predicted to be over 50 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scoring\"></a>\n",
    "## 5. Deploy and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn how to create online scoring and to score a new data record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Science Experience Local user interface will be used to deploy the model that we previously saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>To Do: Capture UI process on DSX Local July release.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is to demonstrate the you can make an external REST API call to test the model\n",
    "\n",
    "    Retreive the bearer token for accessing your deployed model with this command. \n",
    "\n",
    "    Replace <NodeAddress> with the hostname or IP address that this web page notebook is currently using.\n",
    "    Replace <DSXusername> with your DSX Local userid.\n",
    "    Replace <DSXpassword> with your DSX Local password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl -k -X GET https://<NodeAddress>/v2/identity/token -H \"username: <DSXusername>\" -H \"password: <DSXpassword>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score,\n",
    "\n",
    "    Replace the <BearerToken> with the \"accessToken\" value above.\n",
    "\n",
    "    Replace the <InternalScoringEndpoint> with the internal scoring endpoint, e.g. https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443/v2/scoring/online/e09458b4-e18a-457f-bbdd-3302278f6cb0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl -i -k -X POST <InternalScoringEndpoint> -d '{\"fields\": [\"GENDER\",\"AGE\",\"MARITAL_STATUS\",\"PROFESSION\"],\"records\": [[\"M\",23,\"Single\",\"Student\"]]}' -H \"content-type: application/json\" -H \"authorization: Bearer <accessToken>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we predict that a 23-year-old male student is interested in personal accessories (predictedLabel: Personal Accessories, prediction: 1.0). \n",
    "\n",
    "This REST call could be leveraged by applications to perform real-time scoring with a friendly user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary and next steps     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " You successfully completed this notebook! You learned how to use Apache Spark machine learning for model creation and deployment. Check out our _[Online Documentation](https://console.ng.bluemix.net/docs/services/PredictiveModeling/pm_service_api_spark.html)_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "This notebook is based on the work of Lukasz Cmielowski, PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 with DSX Spark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
